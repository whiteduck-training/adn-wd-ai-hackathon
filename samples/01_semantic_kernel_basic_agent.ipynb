{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Azure AI Hackathon ‚Äì Semantic Kernel AI Agent Sample\n",
    "\n",
    "Welcome to the **Azure AI Hackathon**! In this code sample, you will learn how to build a basic AI agent using the [Semantic Kernel](https://aka.ms/ai-agents-beginners/semantic-kernel) AI framework. The **Semantic Kernel** provides a powerful foundation for integrating Large Language Models (LLMs) into your applications with minimal effort.\n",
    "\n",
    "\n",
    "## üéØ **Objective**  \n",
    "\n",
    "The goal of this sample is to demonstrate how to:  \n",
    "- Implement a simple AI agent using **Semantic Kernel**.  \n",
    "- Integrate with **GitHub Copilot Models** for free LLM access during development.  \n",
    "- Seamlessly transition to **Azure OpenAI Service** for enterprise-grade scalability and reliability.  \n",
    "\n",
    "\n",
    "## üõ†Ô∏è **Why GitHub Copilot Models First?**  \n",
    "For development and experimentation, GitHub Copilot Models are a great choice because:  \n",
    "- They provide free access to powerful language models.  \n",
    "- They allow rapid prototyping without initial cost.\n",
    "\n",
    "However, for production environments, **GitHub Copilot Models** may not be suitable due to the lack of:  \n",
    "- **Service Level Agreements (SLAs)**  \n",
    "- **Scalability guarantees**  \n",
    "- **Higher throughput requirements**  \n",
    "\n",
    "\n",
    "## üåê **Switching to Azure OpenAI Service**  \n",
    "When moving to production, you should use **Azure OpenAI Service**, which offers:  \n",
    "- **Guaranteed SLA** for mission-critical applications.  \n",
    "- **Higher throughput** and performance for enterprise workloads.  \n",
    "- **Robust security and compliance** features required in professional environments.  \n",
    "\n",
    "In this sample, you will **swap out the LLM model** from GitHub Copilot to Azure OpenAI with minimal code changes. This flexibility is **crucial** for modern AI applications because:  \n",
    "- New, more efficient models are frequently released.  \n",
    "- Upgrading to better or cheaper models should not require major refactoring.  \n",
    "- Future-proofing your AI applications ensures long-term maintainability.  \n",
    "\n",
    "## üí° **Key Takeaways from this Sample**  \n",
    "- Build modular AI agents with **Semantic Kernel**.  \n",
    "- Design AI solutions that can easily switch between LLM providers.  \n",
    "- Understand the difference between development-grade and production-grade AI services.  \n",
    "- Learn best practices for scalable, maintainable AI application development.  \n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from rich.console import Console\n",
    "from rich.prompt import Prompt\n",
    "from rich.json import JSON\n",
    "\n",
    "console = Console()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê **Integrating Azure Inference SDK with Semantic Kernel**  \n",
    "\n",
    "To connect with GitHub Models, we will use the **Azure Inference SDK**, which provides the `base_url` configuration needed for model access. For this integration, we will use the **`AsyncOpenAI` connector** within **Semantic Kernel**.\n",
    "\n",
    "\n",
    "#### ‚ö° **Why Use the `AsyncOpenAI` Connector?**  \n",
    "- ‚ö° Supports **asynchronous requests** for efficient and scalable processing.  \n",
    "- üîó **Seamless integration** with GitHub Models.  \n",
    "- üîÑ **Flexible enough** to switch between various LLM providers with minimal code changes.  \n",
    "\n",
    "üí¨ *If you want to explore additional connectors for other LLM providers, check out the [Semantic Kernel AI services documentation](https://learn.microsoft.com/semantic-kernel/concepts/ai-services/chat-completion).*\n",
    "\n",
    "\n",
    "### üèóÔ∏è **Creating and Configuring a Kernel**  \n",
    "\n",
    "The **kernel** is the core component in **Semantic Kernel**, responsible for managing AI services and plugins that your AI agents will use.\n",
    "\n",
    "\n",
    "#### üí° **What is a Kernel?**  \n",
    "- üì¶ A **Kernel** is essentially a container for all AI services (like LLMs) and custom plugins.  \n",
    "- ü§ñ It allows your AI agent to **orchestrate multiple services seamlessly**, providing a unified interface for complex AI workflows.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ[\"GITHUB_TOKEN\"], base_url=\"https://models.inference.ai.azure.com/\")\n",
    "\n",
    "kernel = Kernel()\n",
    "chat_completion_service = OpenAIChatCompletion(\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    async_client=client,\n",
    "    service_id=\"agent\",\n",
    ")\n",
    "kernel.add_service(chat_completion_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **Try Different Models**  \n",
    "\n",
    "You can experiment by changing the `ai_model_id` to other models available in the [GitHub Models marketplace](https://aka.ms/ai-agents-beginners/github-models). Doing so will help you:  \n",
    "\n",
    "- ‚úÖ **Compare outputs** from different models.  \n",
    "- üöÄ **Understand model performance** in various tasks.  \n",
    "- üí° **Evaluate cost and efficiency** based on your specific requirements.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ú® **Define Your AI Agent**\n",
    "\n",
    "Let's start by defining a **name** and **instructions** for your AI agent. This step is crucial because it sets the tone and purpose for how your agent will interact and respond.  \n",
    "\n",
    "üí° **Be creative** and personalize your agent! Think about what tasks it should handle and how it should behave. Should it be formal, friendly, humorous, or highly technical? The possibilities are endless.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìù **Example:**\n",
    "```python\n",
    "AGENT_NAME = \"InsightBot\"\n",
    "AGENT_INSTRUCTIONS = \"You are InsightBot, a helpful and knowledgeable AI assistant. Provide clear, concise, and insightful responses. Always maintain a friendly and professional tone. If you don't know an answer, admit it rather than providing incorrect information.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_NAME = \"my_agent\" # TODO: Give your Agent a name\n",
    "AGENT_INSTRUCTIONS = \"You are an emoji translator that translates every text into pure emojis\" # TODO: Give your Agent some instructions\n",
    "agent = ChatCompletionAgent(service_id=\"agent\", kernel=kernel, name=AGENT_NAME, instructions=AGENT_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agents \n",
    "\n",
    "Now we can run the Agent by defining the `ChatHistory` and adding the `system_message` to it. We will use the `AGENT_INSTRUCTIONS` that we defined earlier. \n",
    "\n",
    "After these are defined, we create a `user_inputs` that will be what the user is sending to the agent. In this case, we have set this message to `Plan me a sunny vacation`. \n",
    "\n",
    "Feel free to change this message to see how the agent responds differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Define the chat history\n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_system_message(AGENT_INSTRUCTIONS)\n",
    "\n",
    "    user_input = Prompt.ask(\"User\", default=\"\")\n",
    "    user_inputs = [\n",
    "        user_input,\n",
    "    ]\n",
    "    for user_input in user_inputs:\n",
    "        # Add the user input to the chat history\n",
    "        chat_history.add_user_message(user_input)\n",
    "        console.print(f\"# User: '{user_input}'\")\n",
    "        # Invoke the agent to get a response\n",
    "        async for content in agent.invoke(chat_history):\n",
    "            # Add the response to the chat history\n",
    "            chat_history.add_message(content)\n",
    "            console.print(f\"# Agent - {content.name or '*'}: '{content.content}'\")\n",
    "\n",
    "        user_inputs.append(Prompt.ask(\"User\", default=\"\"))\n",
    "        \n",
    "\n",
    "# For Jupyter notebooks, use this instead of asyncio.run():\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
